(10-05-05)
Notation illustraion:
	-e: max_size, 'inf' means no cutoff
	-d: density cutoff 
	-u: unknown_cut_off, 1 = no cutoff
	-p: p_value_cut_off, 1 = no cutoff
	-m: merge redundant clusters, 1=enable, 0=disable
	result: accuracy/known_predictions/unknown_predictions/
		accuracy_pair/known-predictions-pair/unknown-predictions-pair/known-genes/unknown-genes
			xxx-pair is the non-redundant counter of xxx.
			redundancy means same prediction(gene_no:go_no) is counted >1.


1. schema: hs_fim_40
	40 datasets
	7277/9237 (known/all genes)
	154194 edges(min support=4)

1.1. edge recurrence >=4 and <=40

No.	-e	-d	-u	-p	-m	result
1	inf	0	1	1	0	0.7004/14094/1190/0.6010/1208/118/539/64
2	40	0	1	1	0	0.7263/20211/1989/0.5995/1508/151/604/74
3	inf	0.5	1	1	0	0.7730/27431/2365/0.6010/1812/194/549/69
4	40	0.5	1	1	0	0.7738/25701/2256/0.6007/1763/185/533/66
5	20	0	1	1	0	0.7203/13298/1221/0.6001/1478/145/642/77
6	60	0	1	1	0	0.7254/19649/1775/0.6007/1212/133/548/73
7	40	0	0.2	0.01	0	0.7456/24956/2698/0.5998/1112/119/450/47
8	40	0	0.2	0.01	1	0.6000/165/9/0.6000/165/9/139/8
9	40	0	1	1	1	0.5985/274/23/0.5985/274/23/222/19
10	inf	0	0.2	0.01	0	0.6964/9784/1168/0.6000/780/72/355/35
11	inf	0	0.2	0.01	1	0.5966/238/7/0.5966/238/7/186/6
12	inf	0	1	1	1	0.6008/263/25/0.6008/263/25/212/21
13	40	0	1	0.01	0	0.7450/24365/2522/0.5998/1107/121/451/51
14	inf	0	1	0.01	0	0.6815/8144/834/0.5997/702/66/353/34
15	40	0	0.2	1	0	0.7309/17520/1844/0.5993/1093/114/470/50
16	inf	0	0.2	1	0	0.7146/9038/809/0.6021/862/81/387/41

Conclusion:
	. size has effect. It's still right that smaller clusters are better. See No. 1 vs 2,5,6, 13 vs 14.
	. density's effect is less strong than size. See No 1 vs 3, 2 vs 4.
	. Using size and density cutoff alone has effect. But combining them together is as obivous as alone, which indicate they are dependent. See No. 1 vs 3, 2 vs 4, 1 vs 4.
	. p-value cutoff is not good. See No. 2 vs 13, 1 vs 14.
	. unknown cutoff is not good as well. See No. 2 vs 15, 1 vs 16, 7 vs 13, 10 vs 14.
	. unknown cutoff + p-value cutoff is a bad idea. It reduces the prediction pool(both good and bad predictions) too much. See No.2 vs 7, 1 vs 10.
	. merge is also a bad idea. It reduces the prediction pool even further. See No 7 vs 8, 2 vs 9, 10 vs 11, 1 vs 12.


Suggestion:
	. Our model considers p-value, recurrence, density, but NO size. So it's no wonder that size has some effect. Unknown gene ratio having bad effect is also understandable. The model already selects the predictions to meet the accuracy cutoff and known genes contribute to the accuracy.
	. So we could consider size into model to see something. Previously i dropped it because of its strong association with density. So maybe size is stronger than density and we should substitute density with size or keep them both.

1.1.1. playing with linear model(10-06-05)

Additional Notation
	bit: binary string corresponds to [p_value, recurrence, connectivity, cluster_size]. i.e. 111 means p_value, recurrence and connectivity. 1101 means p_value, recurrence and cluster_size.
	(others see the beginning illustration)

Default setting:
	-d: 0
	-u: 1
	-p: 1
	-m: 0

No.	-e	bit	result
1	40	111	0.7263/20211/1989/0.5995/1508/151/604/74
2	40	1101	0.7468/27161/3131/0.5998/1287/132/512/53
3	40	1111	0.7317/26851/2820/0.5996/1711/172/649/80
4	20	1111	0.7230/14994/1465/0.6010/1441/140/595/72
5	60	1111	0.7363/24301/2245/0.5992/1325/137/563/75
6	inf	1111	0.7039/13628/1145/0.6002/1208/120/536/64
1.1->5	20	111	0.7203/13298/1221/0.6001/1478/145/642/77
1.1->6	60	111	0.7254/19649/1775/0.6007/1212/133/548/73
1.1->1	inf	111	0.7004/14094/1190/0.6010/1208/118/539/64
7	20	1101	0.7266/11315/1243/0.6014/986/98/441/46
8	60	1101	0.7454/28456/3079/0.5991/1080/118/451/51
9	inf	1101	0.7357/24982/2765/0.6008/1017/87/424/41

Conclusion:
	. size alone is bad, worse than connectivity alone. See No 1 vs 2, 2 vs 3, 1.1->5 vs 7, 1.1->6 vs 8, 1.1->1 vs 9.
	. size + connectivity is better than connectivity alone in -e 40. See No. 1 vs 3. But in -e 20, 60 or inf, almost no difference.

1.2 playing with qualified edge occurrence (10-06-05)

Additional Notation:
	m: (diff. from -m), minimum edge occurrence
	x: maximum edge occurrence

Default setting:
	-d: 0
	-u: 1
	-p: 1
	-m: 0
	
No.	-e	m	x	result
1.	40	4	10	0.7595/21938/1849/0.5999/2632/269/867/95
2.	inf	4	10	0.7293/22481/1834/0.5946/2558/267/861/108
3.	40	4	20	0.7263/20211/1989/0.5995/1508/151/604/74
4.	inf	4	20	0.7039/13628/1145/0.6002/1208/120/536/64
5.	40	4	30	0.7263/20211/1989/0.5995/1508/151/604/74
6.	inf	4	30	0.7039/13628/1145/0.6002/1208/120/536/64
1.1->2	40	4	40	0.7263/20211/1989/0.5995/1508/151/604/74
1.1->1	inf	4	40	0.7004/14094/1190/0.6010/1208/118/539/64


Conclusion:
	. smaller maximum occurrence gives more predictions. It's like, less datasets, more predictions. Confirms the situation between mm_fim_97 and mm_fim_65.
 See. No 1 vs 3,5,1.1->2;2 vs 4,6,1.1->1.
 	. Smaller size still makes more predictions. 1 vs 2 is an exception. 2 has more unknown genes but less known genes.
	. complicated linear model makes things weird.

1.3 simple cluster cut off based on accuracy>=0.6 (10-07-05)

Notation:
	-a: minimum accuracy for a cluster to be taken into account
	lm: linear model indicator


Default setting:
	-e: 40
	-d: 0
	-u: 1
	-p: 1
	-m: 0

No.	-a	lm	results
1	0.6	0	0.7853/74078/3368/0.6293/7020/328/1866/156
1.1->1	0	1	0.7004/14094/1190/0.6010/1208/118/539/64


2. schema: mm_fim_97
	97 datasets
	9131/18803 (known/all genes)
	805253 edges (min support=4)

2.1 edge recurrence >=4 and <= 200 (10-05-05)

Notation illustration:
	-a: accuracy cutoff

No	-a	result
1	0.4	0.7224/12525618/2365702/0.3686/4865/1440/1008/339
2	0.5	0.7298/1822797/323591/0.4685/1808/469/424/132
3	0.6	0.7656/142556/22972/0.5661/613/141/149/44

Conclusion:
	. accuracy is the most strong influencer of coverage. (They conflict)
	
Suggestion:
	. If we want to adjust any parameter(size, density, p-value, unknown), we are hoping that it has a strong positive relationship with accuracy. But some are true, some are partially true, some are wrong. Linear model helps us to balance the equation. However, another route is that we just use the accuracy of the each cluster to cut. In this way, we can keep the large pool(coverage) and sound accuracy.
