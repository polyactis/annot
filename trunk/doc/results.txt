(10-05-05)
Notation illustraion:
	-e: max_size, 'inf' means no cutoff
	-d: density cutoff 
	-u: unknown_cut_off, 1 = no cutoff
	-p: p_value_cut_off, 1 = no cutoff
	-m: merge redundant clusters, 1=enable, 0=disable
	result: accuracy/known_predictions/unknown_predictions/
		accuracy_pair/known-predictions-pair/unknown-predictions-pair/known-genes/unknown-genes
			xxx-pair is the non-redundant counter of xxx.
			redundancy means same prediction(gene_no:go_no) is counted >1.


1. schema: hs_fim_40
	40 datasets
	7277/9237 (known/all genes)
	154194 edges(min support=4)

1.1. edge recurrence >=4 and <=40

the no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40_e5_a60
2.	hs_fim_40m4x40ms40_e5_a60
3.	hs_fim_40m4x40d50_e5_a60
4.	hs_fim_40m4x40d50ms40_e5_a60
5.	hs_fim_40m4x40ms20_e5_a60
6.	hs_fim_40m4x40ms60_e5_a60
7.	hs_fim_40m4x40ms40u2p01_e5_a60
8.	hs_fim_40m4x40ms40u2p01m_e5_a60
9.	hs_fim_40m4x40ms40m_e5_a60
10.	hs_fim_40m4x40ms12000p01u2_e5_a60
11.	hs_fim_40m4x40ms12000p01u2m_e5_a60
12.	hs_fim_40m4x40ms12000m_e5_a60
13.	hs_fim_40m4x40ms40p01_e5_a60
14.	hs_fim_40m4x40ms12000p01_e5_a60
15.	hs_fim_40m4x40ms40u2_e5_a60
16.	hs_fim_40m4x40ms12000u2_e5_a60

No.	-e	-d	-u	-p	-m	result
1	inf	0	1	1	0	0.7004/14094/1190/0.6010/1208/118/539/64
2	40	0	1	1	0	0.7263/20211/1989/0.5995/1508/151/604/74
3	inf	0.5	1	1	0	0.7730/27431/2365/0.6010/1812/194/549/69
4	40	0.5	1	1	0	0.7738/25701/2256/0.6007/1763/185/533/66
5	20	0	1	1	0	0.7203/13298/1221/0.6001/1478/145/642/77
6	60	0	1	1	0	0.7254/19649/1775/0.6007/1212/133/548/73
7	40	0	0.2	0.01	0	0.7456/24956/2698/0.5998/1112/119/450/47
8	40	0	0.2	0.01	1	0.6000/165/9/0.6000/165/9/139/8
9	40	0	1	1	1	0.5985/274/23/0.5985/274/23/222/19
10	inf	0	0.2	0.01	0	0.6964/9784/1168/0.6000/780/72/355/35
11	inf	0	0.2	0.01	1	0.5966/238/7/0.5966/238/7/186/6
12	inf	0	1	1	1	0.6008/263/25/0.6008/263/25/212/21
13	40	0	1	0.01	0	0.7450/24365/2522/0.5998/1107/121/451/51
14	inf	0	1	0.01	0	0.6815/8144/834/0.5997/702/66/353/34
15	40	0	0.2	1	0	0.7309/17520/1844/0.5993/1093/114/470/50
16	inf	0	0.2	1	0	0.7146/9038/809/0.6021/862/81/387/41

Conclusion:
	. size has effect. It's still right that smaller clusters are better. See No. 1 vs 2,5,6, 13 vs 14.
	. density's effect is less strong than size. See No 1 vs 3, 2 vs 4.
	. Using size and density cutoff alone has effect. But combining them together is as obivous as alone, which indicate they are dependent. See No. 1 vs 3, 2 vs 4, 1 vs 4.
	. p-value cutoff is not good. See No. 2 vs 13, 1 vs 14.
	. unknown cutoff is not good as well. See No. 2 vs 15, 1 vs 16, 7 vs 13, 10 vs 14.
	. unknown cutoff + p-value cutoff is a bad idea. It reduces the prediction pool(both good and bad predictions) too much. See No.2 vs 7, 1 vs 10.
	. merge is also a bad idea. It reduces the prediction pool even further. See No 7 vs 8, 2 vs 9, 10 vs 11, 1 vs 12.


Suggestion:
	. Our model considers p-value, recurrence, density, but NO size. So it's no wonder that size has some effect. Unknown gene ratio having bad effect is also understandable. The model already selects the predictions to meet the accuracy cutoff and known genes contribute to the accuracy.
	. So we could consider size into model to see something. Previously i dropped it because of its strong association with density. So maybe size is stronger than density and we should substitute density with size or keep them both.

1.1.1. playing with linear model(10-06-05)

Additional Notation
	bit: binary string corresponds to [p_value, recurrence, connectivity, cluster_size]. i.e. 111 means p_value, recurrence and connectivity. 1101 means p_value, recurrence and cluster_size.
	(others see the beginning illustration)

Default setting:
	-d: 0
	-u: 1
	-p: 1
	-m: 0

No.	-e	bit	result
1	40	111	0.7263/20211/1989/0.5995/1508/151/604/74
2	40	1101	0.7468/27161/3131/0.5998/1287/132/512/53
3	40	1111	0.7317/26851/2820/0.5996/1711/172/649/80
4	20	1111	0.7230/14994/1465/0.6010/1441/140/595/72
5	60	1111	0.7363/24301/2245/0.5992/1325/137/563/75
6	inf	1111	0.7039/13628/1145/0.6002/1208/120/536/64
1.1->5	20	111	0.7203/13298/1221/0.6001/1478/145/642/77
1.1->6	60	111	0.7254/19649/1775/0.6007/1212/133/548/73
1.1->1	inf	111	0.7004/14094/1190/0.6010/1208/118/539/64
7	20	1101	0.7266/11315/1243/0.6014/986/98/441/46
8	60	1101	0.7454/28456/3079/0.5991/1080/118/451/51
9	inf	1101	0.7357/24982/2765/0.6008/1017/87/424/41

Conclusion:
	. size alone is bad, worse than connectivity alone. See No 1 vs 2, 2 vs 3, 1.1->5 vs 7, 1.1->6 vs 8, 1.1->1 vs 9.
	. size + connectivity is better than connectivity alone in -e 40. See No. 1 vs 3. But in -e 20, 60 or inf, almost no difference.

1.2 playing with qualified edge occurrence (10-06-05)

Additional Notation:
	m: (diff. from -m), minimum edge occurrence
	x: maximum edge occurrence

Default setting:
	-d: 0
	-u: 1
	-p: 1
	-m: 0
	
No.	-e	m	x	result
1.	40	4	10	0.7595/21938/1849/0.5999/2632/269/867/95
2.	inf	4	10	0.7293/22481/1834/0.5946/2558/267/861/108
3.	40	4	20	0.7263/20211/1989/0.5995/1508/151/604/74
4.	inf	4	20	0.7039/13628/1145/0.6002/1208/120/536/64
5.	40	4	30	0.7263/20211/1989/0.5995/1508/151/604/74
6.	inf	4	30	0.7039/13628/1145/0.6002/1208/120/536/64
1.1->2	40	4	40	0.7263/20211/1989/0.5995/1508/151/604/74
1.1->1	inf	4	40	0.7004/14094/1190/0.6010/1208/118/539/64


Conclusion:
	. smaller maximum occurrence gives more predictions. It's like, less datasets, more predictions. Confirms the situation between mm_fim_97 and mm_fim_65.
 See. No 1 vs 3,5,1.1->2;2 vs 4,6,1.1->1.
 	. Smaller size still makes more predictions. 1 vs 2 is an exception. 2 has more unknown genes but less known genes.
	. complicated linear model makes things weird.

1.3 simple cluster cut off based on accuracy>=0.6 (10-07-05)

Notation:
	-a: minimum accuracy for a cluster to be taken into account
	lm: linear model indicator


Default setting:
	-e: 40
	-d: 0
	-u: 1
	-p: 1
	-m: 0

the no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40mca6_e5_a60

No.	-a	lm	results
1	0.6	0	0.7853/74078/3368/0.6293/7020/328/1866/156
1.1->1	0	1	0.7004/14094/1190/0.6010/1208/118/539/64


Conclusion:
	. a lot potential there. We are doing it in a right way. The coverage vs
		acc_cut_off curve is far below the optimism.

1.3.1 (10-13-05, update) the above result is based on the wrong formula to
	calculate the accuracy. The wrong formula is (#correct-#unknown)/#total.
	The correct one is #correct/(#correct+#wrong).

the no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40a0_6_e5_a60

No.	-a	lm	results
1	0.6	0	0.7444/136195/14864/0.5765/11555/1942/2343/430

1.4 vertex_gradient and edge_gradient added to the model to see effect (10-10-05)

Notation:
	lm_bit: p-value, recurrence, connectivity, cluster_size, edge_gradient, vertex_gradient
	(different from other places)

vertex_gradient and edge_gradient are using exponent 1; score 1,-1,0; layer 10.
edge_gradient's score is the multiplication of score of vertex 1 and vertex 2.
	(if both -1, reverse the sign).

Default setting:
	-e: inf
	-d: 0
	-u: 1
	-p: 1
	-m: 0

the no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40vg_e5_110001a60 (this is different, and never used)
2.	hs_fim_40m4x40eg_e5_11001a60
3.	hs_fim_40m4x40eg_e5_11011a59
4.	hs_fim_40m4x40g_e5_11101a59
5.	hs_fim_40m4x40vg_e5_11111a59
6.	hs_fim_40m4x40eg_e5_11001a50
7.	hs_fim_40m4x40eg_e5_11001a40
8.	hs_fim_40m4x40_e5_a50
9.	hs_fim_40m4x40_e5_a40
10.	hs_fim_40m4x40_e5_a30
11.	hs_fim_40m4x40_e5_a59

No.	-a(lm)	lm_bit	results
1.	0.6	110001	0.7424/20713/2393/0.6166/973/83/387/37
2.	0.6	11001	0.7401/19136/2253/0.6187/889/73/360/31
3.	0.59	11011	0.7323/20022/2363/0.6065/859/73/377/34
4.	0.59	11101	0.7061/17933/1498/0.5864/1400/135/609/69
5.	0.59	11111	0.7080/19310/1644/0.5813/1538/154/641/75
1.1.1->6	0.6	111100	0.7039/13628/1145/0.6002/1208/120/536/64
6.	0.5	11001	0.7344/67043/6044/0.4996/2452/283/872/118
7.	0.4	11001	0.6762/114766/11448/0.4003/6205/887/1491/254
8.	0.5	11100	0.7188/80493/7141/0.5006/4748/554/1344/198
9.	0.4	11100	0.6292/183995/19868/0.4000/15342/2200/2273/410
10.	0.3	11100	0.5085/367656/53037/0.3007/41214/7206/3613/704
11.	0.59	11000	0.7452/30917/3208/0.5899/1251/109/483/49
1.1->1	0.6	11100	0.7004/14094/1190/0.6010/1208/118/539/64
1.3->1	0.6(cl)	00000	0.7853/74078/3368/0.6293/7020/328/1866/156
(10-13-05)
1.3.1->1	0.6(cl)	000000	0.7444/136195/14864/0.5765/11555/1942/2343/430


Table to compare average parameters between different settings:

No.\param	rec	conn	size	unknown	p_value
1		7.689	0.336	11.366	0.211	0.00356
2		7.79	0.312	12.497	0.205	0.00287
11		7.47	0.324	12.579	0.199	0.00353
1.1->1		7.09	0.47	8.608	0.220	0.00569
3		7.82	0.30	14.125	0.206	0.00290
4		7.03	0.453	9.16	0.215	0.00525
5		6.97	0.447	8.98	0.217	0.00544
1.3->6		5.72	0.33	10.958	0.163	0.00979
(10-13-05)
1.3.1->1	5.649	0.314	12.064	0.220	0.02146

Conclusion:
	. The problem is that coverage vs acc_cut_off curve of 1.1->1 and
		No. 2 is lower than 1.1.1->6. Under similar p-value, similar
		recurrence, similar connectivity, different clusters are
		still quite different in terms of accuracy.
	. 1st over-penalizing parameter is p-value(hypergeometric). 1.3->6
		has such a high average p-value(the higher, lower its
		contribution because it's -log(p-value)). The p-value doesn't
		distinguish good and bad clusters in the middle range and
		makes the linear model to choose very low cutoff.
		[SUGGESTION]
		Redefine it.
	. 2nd over-penalizing focuses on recurrence, which means recurrence's
		idea is not clear and not distinguish good and bad clusters
		well enough, we need to	redefine it.
		[SUGGESTION]
		The current recurrence is got by summing individual occurrence,
		no power/exponent, each occurrence is got by divide no_of_edges
		in individual dataset by no_of_edges of the summary cluster.
		If we power individual occurrence, we can distinguish mixed
		recurrence and clear(close to 0/1) recurrence.
	. The 3rd differing parameter is unknown_gene_ratio. It's rather lower
		in the no-linear-model case.
		[SUGGESTION]
		Maybe we should add this to linear model.
	. gradient model is lower than the connectivity model (No 1,2 vs 1.1->1)
		So gradient model even penalizes more than connectivity model.
		It flats the differences between good and bad clusters. Edge-
		gradient even flats more than vertex-gradient.
		[SUGGESTION]
		We should reduce the penalizing for genes with other function, and
		increase the genes with same function. Currently, it's -1 and
		1 respectively, which is disadvantageous to latter.
		
		This over-penalizing makes clusters similar in the gradient
		measure and makes the model rely more on recurrence and p-value,
		which cuts the results really big. This can be seen by their
		similarities(No 1,2) to No.11.

1.4.1 (10-13-05) correction and update, the above gradient results were wrong.
	(one bug was p_gene_analysis.py omitted edge_gradient and coeff5, it
	caused chimeric effects.)

(meta-info ditto. The ones No. doesn't change are same as 1.4)

No.	-a(lm)	lm_bit	results
1.	0.6	110001	0.7589/47441/4397/0.6003/1226/109/499/51
2.	0.6	11001	0.7419/27138/2936/0.5998/1177/97/446/43
3.	0.59	11011	0.7330/26597/2923/0.5905/1072/91/448/42
4.	0.59	11101	0.7072/17256/1440/0.5902/1430/140/618/68
5.	0.59	11111	
1.1.1->6	0.6	111100	0.7039/13628/1145/0.6002/1208/120/536/64
6.	0.5	11001	
7.	0.4	11001	
1.4->8	0.5	11100	0.7188/80493/7141/0.5006/4748/554/1344/198
1.4->9	0.4	11100	0.6292/183995/19868/0.4000/15342/2200/2273/410
1.4->10	0.3	11100	0.5085/367656/53037/0.3007/41214/7206/3613/704
1.4->11	0.59	11000	0.7452/30917/3208/0.5899/1251/109/483/49

1.4.2 (10-15-05) more exploring with gradient

Notation:
	exp: exponent of layer
	l: maximum layer considered to compute edge_gradient
	score: three numbers corresponding to same-function,other-function
		unknown

(in the middle, there're some wrong settings with lm_suffix like
	...m4x40e1...., no 2 before e. They use exponent as recurrence
	cutoff.)
the no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x402e1s0_5__1_0l10_e5_11001a60
2.	hs_fim_40m4x402e1s1__1_0l10_e5_11001a60
3.	hs_fim_40m4x402e1s2__1_0l10_e5_11001a60
4.	hs_fim_40m4x402e1s3__1_0l10_e5_11001a60
5.	hs_fim_40m4x402e2s1__1_0l10_e5_11001a60
6.	hs_fim_40m4x402e3s1__1_0l10_e5_11001a60

No.	exp	l	score	results
1.	1	10	.5,-1,0	0.7582/33786/3365/0.5998/1197/135/438/54
2.	1	10	1,-1,0	0.7607/34143/3348/0.6001/1438/161/589/72
3.	1	10	2,-1,0	0.7583/31627/3251/0.6007/1643/180/688/82
4.	1	10	3,-1,0	0.7564/30191/3166/0.5995/1658/183/699/86
5.	2	10	1,-1,0	0.7599/41331/3954/0.6006/1690/192/651/82
6.	3	10	1,-1,0	0.7617/44925/4210/0.5991/1786/195/670/85


1.4.3 (10-16-05) new edge-gradient formula
	1. takes the edges between layer 0 and 1 into account.
	2. the operator between L1 and L2 is + now, old is *
	3. there're options for denominator, n(n-1)/2, n, #e, ...

Notation:
	(exp,l,score ditto)
	-rx: recurrence_x number, meaning depends on -w
	-w: recurrence_x_type, 0(just the sum), 1(use -rx as cutoff)
		2(use -rx as exponent)
	-t: 1=n(n-1)/2, 2=n, 3=#e, 4=gradient n(n-1)/2, 0 nothing

the no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40e2s3__1_0l10w1t1_e5_11001a60
2.	hs_fim_40m4x40e2s3__1_0l10w1t2_e5_11001a60
3.	hs_fim_40m4x40e0_5s3__1_0l10w1t2_e5_11001a60
4.	hs_fim_40m4x40e1s3__1_0l10w1t2_e5_11001a60
5.	hs_fim_40m4x40e2s1__1_0l10w1t2_e5_11001a60
6.	hs_fim_40m4x40e3s3__1_0l10w1t2_e5_11001a60
7.	hs_fim_40m4x40e2s3__1_0l10w1t3_e5_11001a60
8.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_00001a60
9.	hs_fim_40m4x40e2s3__1_0l10w1t1n_e5_00001a60
10.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_10000a60
11.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_10001a60
12.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_00100a60
13.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_11001a60
14.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_01001a60
15.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_11000a60
16.	hs_fim_40m4x40e2s3__1_0l10w1t1j_e5_01000a60
17.	hs_fim_40m4x40e2s3__1_0l10w1t1_e5_10000a60
18.	hs_fim_40m4x40e2s3__1_0l10w1t1_e5_10001a60
19.	hs_fim_40m4x40e2s3__1_0l10w1t1_e5_01000a60
20.	hs_fim_40m4x40e2s3__1_0l10w1t1_e5_01001a60
21.	hs_fim_40m4x40e2s3__1_0l10w1t1_e5_11000a60
22.	hs_fim_40m4x40e2s3__1_0l10w1t1_e5_00100a60
(10-17-05)
23.	hs_fim_40m4x40e2s310l10w1t1w2x5_e5_01000a60
24.	hs_fim_40m4x40e2s310l10w1t1t4_e5_11001a60
25.	hs_fim_40m4x40e2s310l10w1t1tr4_e5_11001a60
26.	hs_fim_40m4x40e2s310l10w1t1tr4_e5_00001a60
27.	hs_fim_40m4x40e2s310l10w1t1t4_e5_00001a60
28.	hs_fim_40m4x40e2s310l10w1t1s320_e5_00001a60
29.	hs_fim_40m4x40e2s310l10w1t1e3_e5_00001a60
30.	hs_fim_40m4x40e2s310l10w1t1s31_1_e5_00001a60
31.	hs_fim_40m4x40e2s310l10w1t1l5_e5_00001a60
32.	hs_fim_40m4x40e2s310l10w1t1e7_e5_00001a60
33.	hs_fim_40m4x40e2s310l10w1t1jwhole_e5_00000a0

No.	exp	score	-t	lm_bits	results
-w=1, -rx=0.8, l=10
1.	2	3,-1,0	1	11001	0.7526/31627/3184/0.6001/1928/210/740/93
2.	2	3,-1,0	2	11001	0.7648/48878/4700/0.6000/1785/202/671/82
3.	0.5	3,-1,0	2	11001	0.7565/28752/2920/0.6004/926/111/448/63
4.	1	3,-1,0	2	11001	0.7640/37736/3694/0.6005/1214/145/516/73
5.	2	1,-1,0	2	11001	0.7611/47081/4442/0.6006/1710/200/651/82
6.	3	3,-1,0	2	11001	0.7692/48878/4662/0.6007/1916/217/704/90
7.	2	3,-1,0	3	11001	0.7595/25338/2828/0.5991/1731/190/699/87
(8 is special, -p=0.005, -a=0.5)
8.	2	3,-1,0	1	00001	0.7656/76994/7021/0.5994/5380/646/1599/224

9.	2	3,-1,0	1	00001	0.6949/6823/508/0.5847/1546/171/769/93

(10-16 is also special, -p=0.005, -a=0.5)
10.	2	3,-1,0	1	10000	0.7691/30961/2906/0.5999/7229/1036/1645/267
11.	2	3,-1,0	1	10001	0.7672/67859/6257/0.5995/5468/664/1607/230
12.	2	3,-1,0	1	00100	0.7594/54765/4553/0.5996/6254/753/1689/236
13.	2	3,-1,0	1	11001	0.7672/75688/6998/0.5999/5179/643/1363/186
14.	2	3,-1,0	1	01001	0.7624/78463/6972/0.5991/4647/560/1312/178
15.	2	3,-1,0	1	11000	0.7659/70466/6863/0.5994/5319/740/1140/179
16.	2	3,-1,0	1	01000	0.7185/114429/11976/0.5367/7973/1292/1481/273

17.	2	3,-1,0	1	10000	-1.0000/0/0/-1.0000/0/0/0/0
18.	2	3,-1,0	1	10001	0.7107/8272/597/0.5993/1632/170/786/88
19.	2	3,-1,0	1	01000	0.7675/17983/2132/0.6309/905/101/318/34
20.	2	3,-1,0	1	01001	0.7424/20126/2164/0.6006/1755/193/663/79
21.	2	3,-1,0	1	11000	0.7580/22986/2478/0.5986/882/100/333/36
22.	2	3,-1,0	1	00100	0.7061/27763/2188/0.5654/6326/679/1869/248

No.	-w	exp	score	-t	layer	lm_bits	results
(10-17-05)
23.	2(x=5)	2	3,-1,0	1	10	01000	0.7658/18677/2173/0.5991/1387/169/411/51
24.	1(x=.8)	2	3,-1,0	0	10	11001	0.7660/31628/3228/0.5998/1127/135/472/61
25.	1(x=.8)	2	3,-1,0	4	10	11001	0.7251/17252/1804/0.5994/1323/138/583/67
26.	1(x=.8)	2	3,-1,0	4	10	00001	0.6461/1003/79/0.6016/369/44/273/34
27.	1(x=.8)	2	3,-1,0	0	10	00001	0.7558/1429/95/0.5976/164/18/164/18
28.	1(x=.8)	2	3,-2,0	1	10	00001	0.6879/6271/462/0.5801/1467/164/748/95
29.	1(x=.8)	3	3,-1,0	1	10	00001	0.7012/6835/507/0.5892/1514/166/751/92
30.	1(x=.8)	2	3,-1,-1	1	10	00001	0.6949/6536/475/0.5823/1513/165/757/91
31.	1(x=.8)	2	3,-1,0	1	5	00001	0.6949/6823/508/0.5847/1546/171/769/93
32.	1(x=.8)	7	3,-1,0	1	10	00001	0.7036/7115/529/0.5875/1532/169/756/92
(33 a special ensemble of No.8,10,12,16)
33.	1(x=.8)	2	3,-1,0	1	10	00000	0.7148/135251/14013/0.5349/10989/1707/2037/353

Conclusion:
	. -t=1 is still best, compare with -t=2,3,4,0. No. 1 vs. 2,7,24,25
	  This edge-gradient denominator is used as a normalization factor.
	  Jasmine suggested -t=3(see 1.7, her email). But i realized probl-
	  em,
	  <quote from my email on 10-16-05>
	  One more thing is the denominator of edge-gradient formula. See
	  conclusion of 1.4.3, the '-t' parameter. When i was implementing this, i
	  realized the no_of_edges has a problem. It overlooks the density
	  difference between different clustes. Say, two clusters with same size,
	  but one has much more edges than the other in its core. Because of the
	  no_of_edges denominator, we 'll see little difference in terms of
	  edge-gradient for the genes in the core. You can see n(n-1)/2 is still
	  the best. I tried n, but it's even worse.
	  The point is that no_of_edges is still not the expected number of edges
	  for a cluster. I come up with a gradient n(n-1)/2 denominator and going
	  to test it out.
	  </quote from my email on 10-16-05>
	  Specific example for which -t=3 is bad is two clusters with same amount
	  of edges within one layer, but one has less vertices on that layer. Cer-
	  tainly, the latter should be better in function enrichness.
	  Example for which -t=2 is bad is that it ignores the number of edges in-
	  crease by ^2 of number of vertices. Not plainly ^1.
	  -t=4 is supposed to be good. It can solve both examples above. Probably
	  the big clusters missed by -t=1 come into play and mess the prediction 
	  up(see below about No.17) (Need CHECK)
	. exp is a strong factor. No. 1,3,4,6
	. -p=0.005 and -a=0.5 followed by different lm_bits. No.8,10-16.
	  Rank individual parameter
	    1. recurrence No.16
	    2. p-value No.10
	    3. connectivity No.12
	    4. edge-gradient No.8
i	  But combining any of them is bad. No.11,13-15.
	. No.9 and No.17-22. The most chaotic part is here.
	  Compare them to No.1. If i rank it, it's
	    1. connectivity No.22
	    2. edge-gradient No.9
	    3. recurrence No.19
	    4. p-value No.17
	  (No.17 is a little bit weird. CHECK needed!)
	  Combination is also bad. No.22 is surprisingly good. But in terms
	  of predictions(2nd, 3rd digit in results), it's not big. It's also
	  true for edge-gradient.
	  It means high connectivity or edge-gradient
	  encompasses more DIVERSE clusters than other high parameters!(THIS
	  IS ALSO TRUE in No. 26-32.)

	  (10-17-05)
	. The chaotic two rankings above tell us one thing. After removal of
	  some low-accuracy clusters, every papameter is kind of good. These
	  low-accuracy clusters' accuracy are very confusing in terms of the
	  parameters. They could have very good p-value, recurrence. Connect-
	  ivity and edge-gradient are less affected by them(No.9 and 22). It-
	  's probably saying that topology or gradient is far precise and st-
	  able.
	. No.17's no prediction is found out why. The sub-region with highest
	  p-value(hypergeometric) is still very confusing and don't have
	  consistent 60%. Actually, this sub-region contains lots of big clu-
	  sters. Big cluster's representative function has usually a very lo-
	  w p-value due to its relative high number of associated genes. It
	  turns out that these big clusters could also have lots of genes wi-
	  th other functions(examples showed >half), which drags down the ac-
	  curacy.
	  In short, there's big exaggeration or distortion of function overr-
	  epresentation by hypergeometric p-value.
	. 5 is pretty much enough for max-layer. No. 9 vs 31.
	. exp=7 is more like adding pleitropy, instead of #genes predicted.
	  No. 32.
	. No. 26,27 is pretty bad among No.26-32. This is probably due to the
	  problem similar to No.17(comment above)

1.4.3.1 (10-18-05) edge gradient's formula and denominator

Notation:
	(see 1.4.3's Notation)
	-t has more meanings, see proj200407.tex.
	
No.	lm_suffix
1.	hs_fim_40m4x40e2s310l10w1t1tm4s320_e5_00001a60
2.	hs_fim_40m4x40e2s310l10w1t1tm4_e5_00001a60
3.	hs_fim_40m4x40e2s310l10w1t1tv4_e5_00001a60
4.	hs_fim_40m4x40e2s310l10w1t1tu4_e5_00001a60
5.	hs_fim_40m4x40e2s310l10w1t1t4sep_e5_00001a60
6.	hs_fim_40m4x40e2s310l10w1t1t4sepbug_e5_00001a60
7.	hs_fim_40m4x40e2s310l10w1t1t5_e5_00001a60
(10-20-05)
8.	hs_fim_40m4x40e2s310l10w1t1t4sbq0_7_e5_00001a60
9.	hs_fim_40m4x40e2s310l10w1t1t4sbq0_8_e5_00001a60
10.	hs_fim_40m4x40e2s310l10w1t1t4sbq0_5_e5_00001a60

No.	-w	exp	score	-t	layer	lm_bits	results
1.	1(x=.8)	2	3,-2,0	m4	10	00001	0.7100/969/75/0.6340/418/42/322/37
2.	1(x=.8)	2	3,-1,0	m4	10	00001	0.6816/1250/110/0.5997/572/61/422/48
3.	1(x=.8)	2	3,-1,0	v4	10	00001	0.7027/7908/541/0.5996/1381/161/750/95
4.	1(x=.8)	2	3,-1,0	u4	10	00001	0.7089/2157/142/0.6009/223/24/176/22
(10-19-05, No.5 is buggy.)
5.	1(x=.8)	2	3,-1,0	4sep	10	00001	0.6364/11/1/0.6667/9/1/9/1
6.	1(x=.8)	2	3,-1,0	4sepbug	10	00001	0.7586/37738/3516/0.6006/1565/212/945/145
7.	1(x=.8)	2	3,-1,0	5	10	00001	0.7593/36659/3412/0.6003/1516/199/921/139

(10-20-05)
No.	-q	results
(others default to No.6, No.6's -q=0.6)
8.	0.7	0.7588/34683/3186/0.6006/1555/215/936/151
9.	0.8	0.7523/29022/2495/0.6000/1620/219/917/145
10.	0.5	0.7579/37377/3563/0.6000/1505/214/948/147

1.5 WhyManyParametersAndLinearModelCaptureSoFewGoodClusters(WMPALMCSFGC)?
(10-12-05)
(10-15-05, all the pictures and tables are in ./1.3_1vs1.1_1/)

Compare Clusters from Three Running Settings and the Total Cluster Repositary:

Schema: hs_fim_40
	40 datasets
	7277/9237 (known/all genes)
	154194 edges(min support=4) 
	edge recurrence >=4 and <=40 

Notation
lm_bit: p-value, recurrence, connectivity, cluster_size, vertex_gradient, edge_gradient 
-a(lm):	linear model accuracy cutoff
-a(cl): cluster accuracy cutoff

Default setting:
        -e: inf
        -d: 0
        -u: 1
        -p: 1
        -m: 0

Setting No.	-a(lm)	lm_bit		results
0		0	000000	(no result) total 27108 clusters
1.4->2		0.6	110001	0.7401/19136/2253/0.6187/889/73/360/31
1.1->1		0.6	111000	0.7004/14094/1190/0.6010/1208/118/539/64
1.3->1		0.6(cl)	000000	0.7853/74078/3368/0.6293/7020/328/1866/156

No 0 refers to the total cluster repertoire. Others refer to results in results.txt.


1.5.1 cluster classification:
category\No.	1.1->1	1.3->1	0	1.4->2	#clusters
I		0	1	1	NA	5826
II		1	1	1	NA	1195
III		1	0	1	NA	802
IV		0	0	1	NA	19285
#clusters	1997	7021	27108	1867

. For the sake of conciseness, i omit 1.4->2. It's almost a subset of 1.1->1.
. 1 or 0 means present or not.
. Category I is the clusters with prediction accuracy above 0.6 (good clusters)  but our model fails to capture. 
. Category II is good clusters captured by our model.
. Category III is bad clusters captured by our model.
. Category IV is bad clusters successfully discarded by our model.

Illustration: 
	. following stuff is cluster-centered, one by one
	. first give a table-form information about a cluster. mcl_id is cluster id. recurrence is the sum of recurrence_array with 0.8 cutoff. ... go_no_list is the predicted function list based on the cluster. p_value_list is a list of p-values corresponding to go-nos in go_no_list.
	. each figure is a separate file to show how genes are wired. The filename is $mcl_id_$go_no.png, i.e. 33_89.png. The genes with green circle have the go-no(function) in go_no_list.
	. any category-specific notification is embedded.
	


1.5.2 Category I. select 30 out of total 5826 clusters. Good cluster but missed(False negative).

Linear model failed to pick them because of following:
	. recurrence is rather low, like No. 33, 1789, etc. with only recurrence 4(w/ 0.8 cutoff).
	. some have rather good network topology but high p-value(insignificant) and low-density, i.e. No.22529.
	. some have good topology and low p-value, but unfortunately , low recurrence and connectivity. i.e. No.19383,21525.
	. some path structures. i.e. No. 24898,33.


1.5.3 Category II. select 20 out of total 1195 clusters.
	These clusters are good clusters picked by our algorithm. Why do we pick them?
	. some clusters have high p-value(insignificant), but recurrence_array has lots of low stuff causing the sum to be big. i.e. No.27.
	. some clusters have high density to compensate their other disadvantages(high p-value, low recurrence). i.e. No. 6551, 7170, 27.
	. some have real high recurrence even with 0.8 cutoff. i.e. No. 19876,16659.
	. some clusters are very big resulting very low p-value(10e-25). i.e. No. 21287, 23816.
	. one function competing example: No. 16659


1.5.4 Category III. select 20 out of 802 clusters.
	These are bad clusters but we picked them because of complicated reasons. What are these complicated reasons?
	. fake high recurrence. Look at the recurrence_array, those low recurrences cause the sum to be big(but recurrence sum with 0.8 cutoff is not high). No. 4888, 14163, 26683, 14240, 14447.
	. function competing makes some clusters have low accuracy. i.e. No. 20079, 16, 9512.
	. some are just because of high connectivity(0.8,0.9). i.e. No. 14391, 20530, 22026.
	. some have very high recurrence(not fake, with 0.8 cutoff). i.e. No. 14489, 17042, 26835.


1.5.5 Category IV. select 30 out of 19285 clusters.

NOTE: this category's information is a little bit different because they are not used in final(good) predictions. So the go_no_list and p_value_list are replaced with go_no and p_value which comes from individual intermediate prediction. And "diff_recur" is different from previous recurrence. "diff_recur" is the just the summation of whole recurrence_array, no 0.8 cutoff.

	These are bad clusters our model avoided. Are there some good stuff in it?
	. separate function region(non-overlapping) within one cluster. i.e. No. 679, 1448.
	. there's a center in it, other area is like desert. i.e. No. 23153, 16520, 14951, 3923, 20101, 13855, 9822, 13152, 24135. The desert could be other function or unknown gene. The latter see No. 9822.
	. Path structures i.e. No. 7363, 18995.
	. really bad i.e. No 8700, 12456.


Conclusion:
	. function competing means in one cluster, >=2 functions share large regions of genes, which makes the leave-one-out very sensitive.
	[SUGGESTION]
	We can create a measure about this function-competing phenoma, more competing(overlapping regions but not parent-child under depth 5), harder to do prediction. One leave-one-out could totally change the prediction. But we first need to see if this new measure correlates to the prediction.
	Another thing is that we should think about function prediction for known genes in these clusters. Their neighbors are double-faced. One face has same function as this gene. Another face has (very) different function.

	. we should second-think recurrence. 1. some recurrence arrays are pretty vague between 0 and 1. Those half numbers in the array can cause the sum to be exceptionally big. Shall we discard these bits or penalize them? 2. Some datasets' graphs are very similar, it's no wonder to see some pattern appearing frequently in these datasets. The actual recurrence(I call it degeneracy recurrence) is lower than this. 
	[SUGGESTION]
	For 1, i already asked a question above.
	For 2, construct a graph similarity matrix to show how similar two arbitrary graphs are. And rank the similarity. Given a high recurrence we see if those reponsible underlying graphs are among the high ranks.

	. gradient idea should be carefully tested. Based on clusters from category IV which have centers in them, we should think about it. But the scoring should be really careful.


	. About linear model. We might think that linear model could compensate some disadvantages for a good prediction. But we( at least I) forgot that it could equally compensate for bad predictions. If one bad prediction has one big advantage, it could also be picked by the model. So linear model is just a mathematical theory. How coherently each variable in the model reflects the quality of a prediction is of utmost importance. We should keep the linear-model variables few and of high quality. But sometimes, it's hard(maybe not present) to find some variables to accurately reflect the nature. Then we are doomed.


1.6 (10-13-05) redefine recurrence

Notation:
	lm_bit: p-value, recurrence, connectivity, cluster_size, edge_gradient
	(different from other places)
	exp: expoenent of each entry in recurrence_array


Default setting:
	-e: inf
	-d: 0
	-u: 1
	-p: 1
	-m: 0
	lm_bit: 111(p-value + recurrence + connectivity)

The no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40rece0_1_e5_11100a60
2.	hs_fim_40m4x40rece0_3_e5_11100a60
3.	hs_fim_40m4x40rece0_7_e5_11100a60
4.	hs_fim_40m4x40rece2_e5_11100a60
5.	hs_fim_40m4x40rece3_e5_11100a60
6.	hs_fim_40m4x40rece4_e5_11100a60
7.	hs_fim_40m4x40rece5_e5_11100a60
8.	hs_fim_40m4x40rece6_e5_11100a60
9.	hs_fim_40m4x40rec0_8_e5_11100a60
10.	hs_fim_40m4x40rece7_e5_11100a60
11.	hs_fim_40m4x40rece8_e5_11100a60
12.	hs_fim_40m4x40rece10_e5_11100a60
13.	hs_fim_40m4x40rece12_e5_11100a60
(10-15-05)
14.	hs_fim_40m4x40ree0_2_e5_11100a60
15.	hs_fim_40m4x40ree0_4_e5_11100a60
16.	hs_fim_40m4x40ree0_6_e5_11100a60
17.	hs_fim_40m4x40ree0_9_e5_11100a60

No.		exp	results
1.		0.1	0.6977/5786/429/0.6020/1010/99/595/70
2.		0.3	0.6952/8446/650/0.5998/1252/124/659/79
3.		0.7	0.7000/12612/991/0.5995/1311/126/602/70
4.		2	0.7431/33789/3199/0.6000/1750/188/672/84
5.		3	0.7454/34865/3303/0.6005/1880/206/707/93
6.		4	0.7454/35405/3344/0.5998/2034/217/760/100
7.		5	0.7469/34182/3146/0.5994/2067/220/767/101
8.		6	0.7452/33067/3031/0.5998/2114/230/777/102
9.		0.8/cut	0.7382/34520/3234/0.5994/2164/222/818/100
10.		7	0.7390/29756/2744/0.5950/2032/228/772/107
11.		8	0.7380/28387/2595/0.6000/2035/224/774/105
12.		10	0.7379/26961/2395/0.6007/2036/218/774/103
13.		12	0.7365/25462/2241/0.5995/2025/217/775/99
1.1->1		1	0.7004/14094/1190/0.6010/1208/118/539/64
1.3.1->1	NA	0.7444/136195/14864/0.5765/11555/1942/2343/430
(10-15-05)
14.		0.2/cut	0.6880/5754/489/0.5980/908/89/490/54
15.		0.4/cut	0.7066/14917/1186/0.6000/1620/160/683/85
16.		0.6/cut	0.7233/18689/1664/0.5996/1496/160/639/81
17.		0.9/cut	0.7437/27312/2415/0.6005/2065/225/793/105

Table to compare average parameters between different settings:
(SELECT avg(recurrence),avg(connectivity),avg(size),
	avg(unknown_ratio),avg(p_value_list[1]) from good_cl_table)

No.\param	rec	conn	size	unknown	p_value
1.		6.423	0.609	10.884	0.218	0.009647
2.		6.584	0.567	9.692	0.219	0.008422
3.		6.831	0.503	8.612	0.223	0.006923
4.		7.270	0.387	11.484	0.207	0.004696
5.		7.300	0.388	11.783	0.208	0.005265
6.		7.270	0.391	11.858	0.208	0.005726
7.		7.252	0.397	11.673	0.208	0.006032
8.		7.214	0.402	11.510	0.209	0.006343
9.		7.366	0.397	12.179	0.208	0.006901
10.		7.241	0.410	11.454	0.210	0.005942
11.		7.230	0.415	11.285	0.211	0.005985
12.		7.190	0.422	11.067	0.212	0.006109
13.		7.164	0.429	10.89	0.213	0.006211
1.1->1		7.09	0.47	8.608	0.220	0.00569
1.3.1->1	5.649	0.314	12.064	0.220	0.02146
(10-15-05)
14.		6.742	0.539	7.990	0.240	0.010479
15.		6.748	0.455	7.735	0.226	0.007010
16.		7.175	0.441	9.647	0.216	0.005389
17.		7.172	0.423	11.178	0.211	0.006329


Conclusion:
	. The new definition of recurrence pays off. See No.1 to 13 vs 1.1->1
	  The best one, no.8 outputed 102 unknown genes, 48 more. No 1 to 8,
	  10 to 13 are of the same type, just exponent from 0.1 to 12. No 9
	  uses 0.8 as cutoff for recurrence_array.
	  1. We should look at the last four numbers in a result, which
	     correspond to known-predictions-pair,unknown-predictions-pair,
	     #known genes, #unknown genes.
	     A strong trend is observed in  #unknown genes as the exponent
	     goes higher.(There's a minor oscillation from No.2 to 3. But
	     the predictions-pair numbers are still consistent which is a
	     better index.)
	     1.1->1 might look like an exception. But most data in section
	     1.1 shows #unknown-predictions-pair is around 130 to 140. still
	     consistent.
	  2. No. 6,7,8 look like stationary in terms of #unknown_genes,
	     but #unknown-predictions-pair still shows it's increasing.
	     No.8, 10,11 really goes into stationary, and No 12, 13 goes
	     down.
	  3. No. 9, the one with simple 0.8 cutoff is still so far among the
	     top. Only No. 8,10,11 could be paralleled to it.

	. The average parameters table shows that
	  1. size and unknown ratio are almost same to those of the Simple
	     Cluster Accuracy Cutoff(SCAC) approach.
	  2. In No. 4-9, connectivity is close to SCAC. But No. 1,2,3 (low
	     exponenets) heavily relied on connectivity with relatively low
	     avg recurrence because recurrence's effect was flattened in
	     these settings. This heavy dependence on connectivity became
	     obvious for No. 11,12,13.
	     Among No.1-8,10-13, No. 5 is the turning point of connectivity
	     trend.
	  3. recurrence is a little bit away from SCAC without considering
	     the difference between these definitions. All recurrence is got
	     by averaging after 0.8 cutoff as it's from good_cl_table.
	     Among No.1-8,10-13, No. 5 is also the turning point of
	     recurrence trend.
	     
	     [SUGGESTION]
	     Based on comment 2,3 above plus the result coverage, exponent
	     4 is probably the best exponent to reward high recurrence and
	     penalize the low recurrence.
	     
	  4. most importantly, p-value is still the problem. It's too far
	     away SCAC!!!
	     [SUGGESTION]
	     p-value is now the center of reform.


1.7 (10-15-05)
Categorical comparison between 1.6-9 and 1.4.2-4.
All the pictures and tables are in ./1.6_9vs1.4.2_4/

Comment to below's quote:
	setting 2 she's referring is not 1.4.2-4,
that's a similar setting with 1 as recurrence cutoff. But the
examples she talked about could be found counterparts.

<quote from Jasmine>
I looked over your file, and in fact, most genes in Cat(III)
(false-positives) are correct predictions. So our problem is actually the
Cat I (false-negatives).

In the Execl file of cat.table, if I sort using "edge-gradient" (1st
variable) and "p-value" (2nd variable), I can see that your prediction is
heavily relied on these two variables, especially the edge-gradient. So I
think the scheme we discussed yesterday afternoon makes sense, since we
focus on these two variable in a two-step procedure in order to maximize
the discriminative power of each variable.

Now I have two suggestions regarding to the formula:

(1) we should not use a clustering-coefficient-alike approach. In fact,
the edges dircetly connecting to the vertex under investigation are
important (layer 0), especially for small graphs. In small graphs, many
genes do not have 2nd neighbor but only 1st neighbrs, e.g. only layer 0
and no more layers. This is one of the reasons why we can not catch
Pattern 87886_2207_383.png and pattern 75803_2207_815. Since L(0) is 0,
you should sum up (Le1^a+Le2^a) instead of multiplying them (the
denominator in the weighting factors).

(2) the denominator n(n-1)/2 is too big, and if you sort the cluster size,
you will find that big clusters generally get very small edge-gradient,
e.g. pattern 34482_332_724.png. You should use the number of real edges in
the pattern instead of all POssible edges n(n-1)/2 in a complete graph.

So, now let's try the following scheme: (1) get all clusters with 50%
genes falling into a functionla category, and global p-value less than
0.005. (2) use above modified edge-gradient to make prediction. There are
still many parameters, but I believe after some adjustment, this method
could be more sensitive, certainly this is not gauranteed. Let's see.
</quote from Jasmine>

Observation and conclusion:
	. Aside from Jasmine's claim that prediction heavily relied on
	  p-value and edge-gradient. But the truth is that magnitude of
	  recurrence still counts. i.e.
	  In setting 1.4-2,
	  recurrence's coeff: 0.280. One recurrence-unit's change could
	  affect the score by 0.28.
	  p-value's coeff: 0.00784. One e-9 magnitude change of p-value
	  could affect the score by ~0.16.
	  edge-gradient's coeff: 1.487. One 0.25 change of edge-gradient
	  could affect the score by 0.37.

	  The reason why i choose those units is based on the data
	  distribution. Roughly, recurrence is from 4 to 8; p-value is
	  from 0 to e-35; edge-gradient is from 0 to 1.

	  Furthermore, category 4's recurrence is much about 4,5. In
	  contrast, in other 3 categories, we see lots of 6,7,8.
	  So does edge-gradient and p-value(but p-value is more mixing).
	  
	  Rank three parameters.
	  1. edge-gradient.
	  2. recurrence
	  3. p-value

2. schema: mm_fim_97
	97 datasets
	9131/18803 (known/all genes)
	805253 edges (min support=4)

2.1 edge recurrence >=4 and <= 200 (10-05-05)

Notation illustration:
	-a: accuracy cutoff

No	-a	result
1	0.4	0.7224/12525618/2365702/0.3686/4865/1440/1008/339
2	0.5	0.7298/1822797/323591/0.4685/1808/469/424/132
3	0.6	0.7656/142556/22972/0.5661/613/141/149/44

Conclusion:
	. accuracy is the most strong influencer of coverage. (They conflict)
	
Suggestion:
	. If we want to adjust any parameter(size, density, p-value, unknown), we are hoping that it has a strong positive relationship with accuracy. But some are true, some are partially true, some are wrong. Linear model helps us to balance the equation. However, another route is that we just use the accuracy of the each cluster to cut. In this way, we can keep the large pool(coverage) and sound accuracy.
