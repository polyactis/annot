(10-05-05)
Notation illustraion:
	-e: max_size, 'inf' means no cutoff
	-d: density cutoff 
	-u: unknown_cut_off, 1 = no cutoff
	-p: p_value_cut_off, 1 = no cutoff
	-m: merge redundant clusters, 1=enable, 0=disable
	result: accuracy/known_predictions/unknown_predictions/
		accuracy_pair/known-predictions-pair/unknown-predictions-pair/known-genes/unknown-genes
			xxx-pair is the non-redundant counter of xxx.
			redundancy means same prediction(gene_no:go_no) is counted >1.


1. schema: hs_fim_40
	40 datasets
	7277/9237 (known/all genes)
	154194 edges(min support=4)

1.1. edge recurrence >=4 and <=40

No.	-e	-d	-u	-p	-m	result
1	inf	0	1	1	0	0.7004/14094/1190/0.6010/1208/118/539/64
2	40	0	1	1	0	0.7263/20211/1989/0.5995/1508/151/604/74
3	inf	0.5	1	1	0	0.7730/27431/2365/0.6010/1812/194/549/69
4	40	0.5	1	1	0	0.7738/25701/2256/0.6007/1763/185/533/66
5	20	0	1	1	0	0.7203/13298/1221/0.6001/1478/145/642/77
6	60	0	1	1	0	0.7254/19649/1775/0.6007/1212/133/548/73
7	40	0	0.2	0.01	0	0.7456/24956/2698/0.5998/1112/119/450/47
8	40	0	0.2	0.01	1	0.6000/165/9/0.6000/165/9/139/8
9	40	0	1	1	1	0.5985/274/23/0.5985/274/23/222/19
10	inf	0	0.2	0.01	0	0.6964/9784/1168/0.6000/780/72/355/35
11	inf	0	0.2	0.01	1	0.5966/238/7/0.5966/238/7/186/6
12	inf	0	1	1	1	0.6008/263/25/0.6008/263/25/212/21
13	40	0	1	0.01	0	0.7450/24365/2522/0.5998/1107/121/451/51
14	inf	0	1	0.01	0	0.6815/8144/834/0.5997/702/66/353/34
15	40	0	0.2	1	0	0.7309/17520/1844/0.5993/1093/114/470/50
16	inf	0	0.2	1	0	0.7146/9038/809/0.6021/862/81/387/41

Conclusion:
	. size has effect. It's still right that smaller clusters are better. See No. 1 vs 2,5,6, 13 vs 14.
	. density's effect is less strong than size. See No 1 vs 3, 2 vs 4.
	. Using size and density cutoff alone has effect. But combining them together is as obivous as alone, which indicate they are dependent. See No. 1 vs 3, 2 vs 4, 1 vs 4.
	. p-value cutoff is not good. See No. 2 vs 13, 1 vs 14.
	. unknown cutoff is not good as well. See No. 2 vs 15, 1 vs 16, 7 vs 13, 10 vs 14.
	. unknown cutoff + p-value cutoff is a bad idea. It reduces the prediction pool(both good and bad predictions) too much. See No.2 vs 7, 1 vs 10.
	. merge is also a bad idea. It reduces the prediction pool even further. See No 7 vs 8, 2 vs 9, 10 vs 11, 1 vs 12.


Suggestion:
	. Our model considers p-value, recurrence, density, but NO size. So it's no wonder that size has some effect. Unknown gene ratio having bad effect is also understandable. The model already selects the predictions to meet the accuracy cutoff and known genes contribute to the accuracy.
	. So we could consider size into model to see something. Previously i dropped it because of its strong association with density. So maybe size is stronger than density and we should substitute density with size or keep them both.

1.1.1. playing with linear model(10-06-05)

Additional Notation
	bit: binary string corresponds to [p_value, recurrence, connectivity, cluster_size]. i.e. 111 means p_value, recurrence and connectivity. 1101 means p_value, recurrence and cluster_size.
	(others see the beginning illustration)

Default setting:
	-d: 0
	-u: 1
	-p: 1
	-m: 0

No.	-e	bit	result
1	40	111	0.7263/20211/1989/0.5995/1508/151/604/74
2	40	1101	0.7468/27161/3131/0.5998/1287/132/512/53
3	40	1111	0.7317/26851/2820/0.5996/1711/172/649/80
4	20	1111	0.7230/14994/1465/0.6010/1441/140/595/72
5	60	1111	0.7363/24301/2245/0.5992/1325/137/563/75
6	inf	1111	0.7039/13628/1145/0.6002/1208/120/536/64
1.1->5	20	111	0.7203/13298/1221/0.6001/1478/145/642/77
1.1->6	60	111	0.7254/19649/1775/0.6007/1212/133/548/73
1.1->1	inf	111	0.7004/14094/1190/0.6010/1208/118/539/64
7	20	1101	0.7266/11315/1243/0.6014/986/98/441/46
8	60	1101	0.7454/28456/3079/0.5991/1080/118/451/51
9	inf	1101	0.7357/24982/2765/0.6008/1017/87/424/41

Conclusion:
	. size alone is bad, worse than connectivity alone. See No 1 vs 2, 2 vs 3, 1.1->5 vs 7, 1.1->6 vs 8, 1.1->1 vs 9.
	. size + connectivity is better than connectivity alone in -e 40. See No. 1 vs 3. But in -e 20, 60 or inf, almost no difference.

1.2 playing with qualified edge occurrence (10-06-05)

Additional Notation:
	m: (diff. from -m), minimum edge occurrence
	x: maximum edge occurrence

Default setting:
	-d: 0
	-u: 1
	-p: 1
	-m: 0
	
No.	-e	m	x	result
1.	40	4	10	0.7595/21938/1849/0.5999/2632/269/867/95
2.	inf	4	10	0.7293/22481/1834/0.5946/2558/267/861/108
3.	40	4	20	0.7263/20211/1989/0.5995/1508/151/604/74
4.	inf	4	20	0.7039/13628/1145/0.6002/1208/120/536/64
5.	40	4	30	0.7263/20211/1989/0.5995/1508/151/604/74
6.	inf	4	30	0.7039/13628/1145/0.6002/1208/120/536/64
1.1->2	40	4	40	0.7263/20211/1989/0.5995/1508/151/604/74
1.1->1	inf	4	40	0.7004/14094/1190/0.6010/1208/118/539/64


Conclusion:
	. smaller maximum occurrence gives more predictions. It's like, less datasets, more predictions. Confirms the situation between mm_fim_97 and mm_fim_65.
 See. No 1 vs 3,5,1.1->2;2 vs 4,6,1.1->1.
 	. Smaller size still makes more predictions. 1 vs 2 is an exception. 2 has more unknown genes but less known genes.
	. complicated linear model makes things weird.

1.3 simple cluster cut off based on accuracy>=0.6 (10-07-05)

Notation:
	-a: minimum accuracy for a cluster to be taken into account
	lm: linear model indicator


Default setting:
	-e: 40
	-d: 0
	-u: 1
	-p: 1
	-m: 0

No.	-a	lm	results
1	0.6	0	0.7853/74078/3368/0.6293/7020/328/1866/156
1.1->1	0	1	0.7004/14094/1190/0.6010/1208/118/539/64


Conclusion:
	. a lot potential there. We are doing it in a right way. The coverage vs
		acc_cut_off curve is far below the optimism.

1.3.1 (10-13-05, update) the above result is based on the wrong formula to
	calculate the accuracy. The wrong formula is (#correct-#unknown)/#total.
	The correct one is #correct/(#correct+#wrong).

No.	-a	lm	results
1	0.6	0	0.7444/136195/14864/0.5765/11555/1942/2343/430

1.4 vertex_gradient and edge_gradient added to the model to see effect (10-10-05)

Notation:
	lm_bit: p-value, recurrence, connectivity, cluster_size, edge_gradient, vertex_gradient
	(different from other places)

vertex_gradient and edge_gradient are using exponent 1; score 1,-1,0; layer 10.
edge_gradient's score is the multiplication of score of vertex 1 and vertex 2.(if both -1, reverse the sign).

Default setting:
	-e: inf
	-d: 0
	-u: 1
	-p: 1
	-m: 0

the no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40vg_e5_110001a60 (this is different, and never used)
2.	hs_fim_40m4x40eg_e5_11001a60
3.	hs_fim_40m4x40eg_e5_11011a59
4.	hs_fim_40m4x40g_e5_11101a59
5.	hs_fim_40m4x40vg_e5_11111a59
6.	hs_fim_40m4x40eg_e5_11001a50
7.	hs_fim_40m4x40eg_e5_11001a40
8.	hs_fim_40m4x40_e5_a50
9.	hs_fim_40m4x40_e5_a40
10.	hs_fim_40m4x40_e5_a30
11.	hs_fim_40m4x40_e5_a59

No.	-a(lm)	lm_bit	results
1.	0.6	110001	0.7424/20713/2393/0.6166/973/83/387/37
2.	0.6	11001	0.7401/19136/2253/0.6187/889/73/360/31
3.	0.59	11011	0.7323/20022/2363/0.6065/859/73/377/34
4.	0.59	11101	0.7061/17933/1498/0.5864/1400/135/609/69
5.	0.59	11111	0.7080/19310/1644/0.5813/1538/154/641/75
1.1.1->6	0.6	111100	0.7039/13628/1145/0.6002/1208/120/536/64
6.	0.5	11001	0.7344/67043/6044/0.4996/2452/283/872/118
7.	0.4	11001	0.6762/114766/11448/0.4003/6205/887/1491/254
8.	0.5	11100	0.7188/80493/7141/0.5006/4748/554/1344/198
9.	0.4	11100	0.6292/183995/19868/0.4000/15342/2200/2273/410
10.	0.3	11100	0.5085/367656/53037/0.3007/41214/7206/3613/704
11.	0.59	11000	0.7452/30917/3208/0.5899/1251/109/483/49
1.1->1	0.6	11100	0.7004/14094/1190/0.6010/1208/118/539/64
1.3->1	0.6(cl)	00000	0.7853/74078/3368/0.6293/7020/328/1866/156
(10-13-05)
1.3.1->1	0.6(cl)	000000	0.7444/136195/14864/0.5765/11555/1942/2343/430


Table to compare average parameters between different settings:

No.\param	rec	conn	size	unknown	p_value
1		7.689	0.336	11.366	0.211	0.00356
2		7.79	0.312	12.497	0.205	0.00287
11		7.47	0.324	12.579	0.199	0.00353
1.1->1		7.09	0.47	8.608	0.220	0.00569
3		7.82	0.30	14.125	0.206	0.00290
4		7.03	0.453	9.16	0.215	0.00525
5		6.97	0.447	8.98	0.217	0.00544
1.3->6		5.72	0.33	10.958	0.163	0.00979
(10-13-05)
1.3.1->1	5.649	0.314	12.064	0.220	0.02146

Conclusion:
	. The problem is that coverage vs acc_cut_off curve of 1.1->1 and
		No. 2 is lower than 1.1.1->6. Under similar p-value, similar
		recurrence, similar connectivity, different clusters are
		still quite different in terms of accuracy.
	. 1st over-penalizing parameter is p-value(hypergeometric). 1.3->6
		has such a high average p-value(the higher, lower its
		contribution because it's -log(p-value)). The p-value doesn't
		distinguish good and bad clusters in the middle range and
		makes the linear model to choose very low cutoff.
		[SUGGESTION]
		Redefine it.
	. 2nd over-penalizing focuses on recurrence, which means recurrence's
		idea is not clear and not distinguish good and bad clusters
		well enough, we need to	redefine it.
		[SUGGESTION]
		The current recurrence is got by summing individual occurrence,
		no power/exponent, each occurrence is got by divide no_of_edges
		in individual dataset by no_of_edges of the summary cluster.
		If we power individual occurrence, we can distinguish mixed
		recurrence and clear(close to 0/1) recurrence.
	. The 3rd differing parameter is unknown_gene_ratio. It's rather lower
		in the no-linear-model case.
		[SUGGESTION]
		Maybe we should add this to linear model.
	. gradient model is lower than the connectivity model (No 1,2 vs 1.1->1)
		So gradient model even penalizes more than connectivity model.
		It flats the differences between good and bad clusters. Edge-
		gradient even flats more than vertex-gradient.
		[SUGGESTION]
		We should reduce the penalizing for genes with other function, and
		increase the genes with same function. Currently, it's -1 and
		1 respectively, which is disadvantageous to latter.
		
		This over-penalizing makes clusters similar in the gradient
		measure and makes the model rely more on recurrence and p-value,
		which cuts the results really big. This can be seen by their
		similarities(No 1,2) to No.11.

1.4.1 (10-13-05) correction and update, the above gradient results were wrong.
	(one bug was p_gene_analysis.py omitted edge_gradient and coeff5, it
	caused chimeric effects.)

(meta-info ditto. The ones No. doesn't change are same as 1.4)

No.	-a(lm)	lm_bit	results
1.	0.6	110001	0.7589/47441/4397/0.6003/1226/109/499/51
2.	0.6	11001	0.7419/27138/2936/0.5998/1177/97/446/43
3.	0.59	11011	0.7330/26597/2923/0.5905/1072/91/448/42
4.	0.59	11101	0.7072/17256/1440/0.5902/1430/140/618/68
5.	0.59	11111	
1.1.1->6	0.6	111100	0.7039/13628/1145/0.6002/1208/120/536/64
6.	0.5	11001	
7.	0.4	11001	
1.4->8	0.5	11100	0.7188/80493/7141/0.5006/4748/554/1344/198
1.4->9	0.4	11100	0.6292/183995/19868/0.4000/15342/2200/2273/410
1.4->10	0.3	11100	0.5085/367656/53037/0.3007/41214/7206/3613/704
1.4->11	0.59	11000	0.7452/30917/3208/0.5899/1251/109/483/49

1.4.2 (10-13-05) more exploring with gradient

1.5 WhyManyParametersAndLinearModelCaptureSoFewGoodClusters(WMPALMCSFGC)?
(10-12-05)

Compare Clusters from Three Running Settings and the Total Cluster Repositary:

Schema: hs_fim_40
	40 datasets
	7277/9237 (known/all genes)
	154194 edges(min support=4) 
	edge recurrence >=4 and <=40 

Notation
lm_bit: p-value, recurrence, connectivity, cluster_size, vertex_gradient, edge_gradient 
-a(lm):	linear model accuracy cutoff
-a(cl): cluster accuracy cutoff

Default setting:
        -e: inf
        -d: 0
        -u: 1
        -p: 1
        -m: 0

Setting No.	-a(lm)	lm_bit		results
0		0	000000	(no result) total 27108 clusters
1.4->2		0.6	110001	0.7401/19136/2253/0.6187/889/73/360/31
1.1->1		0.6	111000	0.7004/14094/1190/0.6010/1208/118/539/64
1.3->1		0.6(cl)	000000	0.7853/74078/3368/0.6293/7020/328/1866/156

No 0 refers to the total cluster repertoire. Others refer to results in results.txt.


1.5.1 cluster classification:
category\No.	1.1->1	1.3->1	0	1.4->2	#clusters
I		0	1	1	NA	5826
II		1	1	1	NA	1195
III		1	0	1	NA	802
IV		0	0	1	NA	19285
#clusters	1997	7021	27108	1867

. For the sake of conciseness, i omit 1.4->2. It's almost a subset of 1.1->1.
. 1 or 0 means present or not.
. Category I is the clusters with prediction accuracy above 0.6 (good clusters)  but our model fails to capture. 
. Category II is good clusters captured by our model.
. Category III is bad clusters captured by our model.
. Category IV is bad clusters successfully discarded by our model.

Illustration: 
	. following stuff is cluster-centered, one by one
	. first give a table-form information about a cluster. mcl_id is cluster id. recurrence is the sum of recurrence_array with 0.8 cutoff. ... go_no_list is the predicted function list based on the cluster. p_value_list is a list of p-values corresponding to go-nos in go_no_list.
	. each figure is a separate file to show how genes are wired. The filename is $mcl_id_$go_no.png, i.e. 33_89.png. The genes with green circle have the go-no(function) in go_no_list.
	. any category-specific notification is embedded.
	


1.5.2 Category I. select 30 out of total 5826 clusters. Good cluster but missed(False negative).

Linear model failed to pick them because of following:
	. recurrence is rather low, like No. 33, 1789, etc. with only recurrence 4(w/ 0.8 cutoff).
	. some have rather good network topology but high p-value(insignificant) and low-density, i.e. No.22529.
	. some have good topology and low p-value, but unfortunately , low recurrence and connectivity. i.e. No.19383,21525.
	. some path structures. i.e. No. 24898,33.


1.5.3 Category II. select 20 out of total 1195 clusters.
	These clusters are good clusters picked by our algorithm. Why do we pick them?
	. some clusters have high p-value(insignificant), but recurrence_array has lots of low stuff causing the sum to be big. i.e. No.27.
	. some clusters have high density to compensate their other disadvantages(high p-value, low recurrence). i.e. No. 6551, 7170, 27.
	. some have real high recurrence even with 0.8 cutoff. i.e. No. 19876,16659.
	. some clusters are very big resulting very low p-value(10e-25). i.e. No. 21287, 23816.
	. one function competing example: No. 16659


1.5.4 Category III. select 20 out of 802 clusters.
	These are bad clusters but we picked them because of complicated reasons. What are these complicated reasons?
	. fake high recurrence. Look at the recurrence_array, those low recurrences cause the sum to be big(but recurrence sum with 0.8 cutoff is not high). No. 4888, 14163, 26683, 14240, 14447.
	. function competing makes some clusters have low accuracy. i.e. No. 20079, 16, 9512.
	. some are just because of high connectivity(0.8,0.9). i.e. No. 14391, 20530, 22026.
	. some have very high recurrence(not fake, with 0.8 cutoff). i.e. No. 14489, 17042, 26835.


1.5.5 Category IV. select 30 out of 19285 clusters.

NOTE: this category's information is a little bit different because they are not used in final(good) predictions. So the go_no_list and p_value_list are replaced with go_no and p_value which comes from individual intermediate prediction. And "diff_recur" is different from previous recurrence. "diff_recur" is the just the summation of whole recurrence_array, no 0.8 cutoff.

	These are bad clusters our model avoided. Are there some good stuff in it?
	. separate function region(non-overlapping) within one cluster. i.e. No. 679, 1448.
	. there's a center in it, other area is like desert. i.e. No. 23153, 16520, 14951, 3923, 20101, 13855, 9822, 13152, 24135. The desert could be other function or unknown gene. The latter see No. 9822.
	. Path structures i.e. No. 7363, 18995.
	. really bad i.e. No 8700, 12456.


Conclusion:
	. function competing means in one cluster, >=2 functions share large regions of genes, which makes the leave-one-out very sensitive.
	[SUGGESTION]
	We can create a measure about this function-competing phenoma, more competing(overlapping regions but not parent-child under depth 5), harder to do prediction. One leave-one-out could totally change the prediction. But we first need to see if this new measure correlates to the prediction.
	Another thing is that we should think about function prediction for known genes in these clusters. Their neighbors are double-faced. One face has same function as this gene. Another face has (very) different function.

	. we should second-think recurrence. 1. some recurrence arrays are pretty vague between 0 and 1. Those half numbers in the array can cause the sum to be exceptionally big. Shall we discard these bits or penalize them? 2. Some datasets' graphs are very similar, it's no wonder to see some pattern appearing frequently in these datasets. The actual recurrence(I call it degeneracy recurrence) is lower than this. 
	[SUGGESTION]
	For 1, i already asked a question above.
	For 2, construct a graph similarity matrix to show how similar two arbitrary graphs are. And rank the similarity. Given a high recurrence we see if those reponsible underlying graphs are among the high ranks.

	. gradient idea should be carefully tested. Based on clusters from category IV which have centers in them, we should think about it. But the scoring should be really careful.


	. About linear model. We might think that linear model could compensate some disadvantages for a good prediction. But we( at least I) forgot that it could equally compensate for bad predictions. If one bad prediction has one big advantage, it could also be picked by the model. So linear model is just a mathematical theory. How coherently each variable in the model reflects the quality of a prediction is of utmost importance. We should keep the linear-model variables few and of high quality. But sometimes, it's hard(maybe not present) to find some variables to accurately reflect the nature. Then we are doomed.


1.6 (10-13-05) redefine recurrence

Notation:
	lm_bit: p-value, recurrence, connectivity, cluster_size, edge_gradient
	(different from other places)
	exp: expoenent of each entry in recurrence_array


Default setting:
	-e: inf
	-d: 0
	-u: 1
	-p: 1
	-m: 0
	lm_bit: 111(p-value + recurrence + connectivity)

The no2lm_suffix mapping for each setting (used to track database tables)
No.	lm_suffix
1.	hs_fim_40m4x40rece0_1_e5_11100a60
2.	hs_fim_40m4x40rece0_3_e5_11100a60
3.	hs_fim_40m4x40rece0_7_e5_11100a60
4.	hs_fim_40m4x40rece2_e5_11100a60
5.	hs_fim_40m4x40rece3_e5_11100a60
6.	hs_fim_40m4x40rece4_e5_11100a60
7.	hs_fim_40m4x40rece5_e5_11100a60
8.	hs_fim_40m4x40rece6_e5_11100a60
9.	hs_fim_40m4x40rec0_8_e5_11100a60
10.	hs_fim_40m4x40rece7_e5_11100a60
11.	hs_fim_40m4x40rece8_e5_11100a60
12.	hs_fim_40m4x40rece10_e5_11100a60
13.	hs_fim_40m4x40rece12_e5_11100a60

No.		exp	results
1.		0.1	0.6977/5786/429/0.6020/1010/99/595/70
2.		0.3	0.6952/8446/650/0.5998/1252/124/659/79
3.		0.7	0.7000/12612/991/0.5995/1311/126/602/70
4.		2	0.7431/33789/3199/0.6000/1750/188/672/84
5.		3	0.7454/34865/3303/0.6005/1880/206/707/93
6.		4	0.7454/35405/3344/0.5998/2034/217/760/100
7.		5	0.7469/34182/3146/0.5994/2067/220/767/101
8.		6	0.7452/33067/3031/0.5998/2114/230/777/102
9.		0.8/cut	0.7382/34520/3234/0.5994/2164/222/818/100
10.		7	0.7390/29756/2744/0.5950/2032/228/772/107
11.		8	0.7380/28387/2595/0.6000/2035/224/774/105
12.		10	0.7379/26961/2395/0.6007/2036/218/774/103
13.		12	0.7365/25462/2241/0.5995/2025/217/775/99
1.1->1		1	0.7004/14094/1190/0.6010/1208/118/539/64
1.3.1->1	NA	0.7444/136195/14864/0.5765/11555/1942/2343/430


Table to compare average parameters between different settings:

No.\param	rec	conn	size	unknown	p_value
1.		6.423	0.609	10.884	0.218	0.009647
2.		6.584	0.567	9.692	0.219	0.008422
3.		6.831	0.503	8.612	0.223	0.006923
4.		7.270	0.387	11.484	0.207	0.004696
5.		7.300	0.388	11.783	0.208	0.005265
6.		7.270	0.391	11.858	0.208	0.005726
7.		7.252	0.397	11.673	0.208	0.006032
8.		7.214	0.402	11.510	0.209	0.006343
9.		7.366	0.397	12.179	0.208	0.006901
10.		7.241	0.410	11.454	0.210	0.005942
11.		7.230	0.415	11.285	0.211	0.005985
12.		7.190	0.422	11.067	0.212	0.006109
13.		7.164	0.429	10.89	0.213	0.006211
1.1->1		7.09	0.47	8.608	0.220	0.00569
1.3.1->1	5.649	0.314	12.064	0.220	0.02146

Conclusion:
	. The new definition of recurrence pays off. See No.1 to 13 vs 1.1->1
	  The best one, no.8 outputed 102 unknown genes, 48 more. No 1 to 8,
	  10 to 13 are of the same type, just exponent from 0.1 to 12. No 9
	  uses 0.8 as cutoff for recurrence_array.
	  1. We should look at the last four numbers in a result, which
	     correspond to known-predictions-pair,unknown-predictions-pair,
	     #known genes, #unknown genes.
	     A strong trend is observed in  #unknown genes as the exponent
	     goes higher.(There's a minor oscillation from No.2 to 3. But
	     the predictions-pair numbers are still consistent which is a
	     better index.)
	     1.1->1 might look like an exception. But most data in section
	     1.1 shows #unknown-predictions-pair is around 130 to 140. still
	     consistent.
	  2. No. 6,7,8 look like stationary in terms of #unknown_genes,
	     but #unknown-predictions-pair still shows it's increasing.
	     No.8, 10,11 really goes into stationary, and No 12, 13 goes
	     down.
	  3. No. 9, the one with simple 0.8 cutoff is still so far among the
	     top. Only No. 8,10,11 could be paralleled to it.

	. The average parameters table shows that
	  1. size and unknown ratio are almost same to those of the Simple
	     Cluster Accuracy Cutoff(SCAC) approach.
	  2. In No. 4-9, connectivity is close to SCAC. But No. 1,2,3 (low
	     exponenets) heavily relied on connectivity with relatively low
	     avg recurrence because recurrence's effect was flattened in
	     these settings. This heavy dependence on connectivity became
	     obvious for No. 11,12,13.
	     Among No.1-8,10-13, No. 5 is the turning point of connectivity
	     trend.
	  3. recurrence is a little bit away from SCAC without considering
	     the difference between these definitions. SCAC's number is got
	     by averaging after 0.8 cutoff, so a little bit low.
	     Among No.1-8,10-13, No. 5 is also the turning point of
	     recurrence trend.
	     
	     [SUGGESTION]
	     Based on comment 2,3 above plus the result coverage, exponent
	     4 is probably the best exponent to reward high recurrence and
	     penalize the low recurrence.
	     
	  4. most importantly, p-value is still the problem. It's too far
	     away SCAC!!!
	     [SUGGESTION]
	     p-value is now the center of reform.

2. schema: mm_fim_97
	97 datasets
	9131/18803 (known/all genes)
	805253 edges (min support=4)

2.1 edge recurrence >=4 and <= 200 (10-05-05)

Notation illustration:
	-a: accuracy cutoff

No	-a	result
1	0.4	0.7224/12525618/2365702/0.3686/4865/1440/1008/339
2	0.5	0.7298/1822797/323591/0.4685/1808/469/424/132
3	0.6	0.7656/142556/22972/0.5661/613/141/149/44

Conclusion:
	. accuracy is the most strong influencer of coverage. (They conflict)
	
Suggestion:
	. If we want to adjust any parameter(size, density, p-value, unknown), we are hoping that it has a strong positive relationship with accuracy. But some are true, some are partially true, some are wrong. Linear model helps us to balance the equation. However, another route is that we just use the accuracy of the each cluster to cut. In this way, we can keep the large pool(coverage) and sound accuracy.
